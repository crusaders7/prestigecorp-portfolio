#!/usr/bin/env python3
"""
Deep Pagination Scanner for Missing Articles
Scans deeper into pagination of working categories to find missing articles
"""

import requests
from bs4 import BeautifulSoup
import re
import time

def scan_deep_pagination():
    target_ids = ['9050660', '9046630', '9045329', '636609', '9044604']
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
    
    # Working categories from our comprehensive discovery
    working_categories = [
        'https://www.illawarramercury.com.au/',  # Homepage - most promising
        'https://www.illawarramercury.com.au/sport/',
        'https://www.illawarramercury.com.au/news/',
        'https://www.illawarramercury.com.au/politics/',
        'https://www.illawarramercury.com.au/local-news/',
        'https://www.illawarramercury.com.au/local-sport/',
        'https://www.illawarramercury.com.au/community/',
    ]
    
    print('ğŸ” DEEP PAGINATION SCAN FOR MISSING ARTICLES')
    print('=' * 60)
    
    all_found = []
    
    for category in working_categories:
        print(f'\nğŸ“‚ Scanning: {category}')
        
        for page in range(1, 11):  # Scan up to 10 pages deep
            page_url = f"{category}?page={page}" if page > 1 else category
            
            try:
                response = requests.get(page_url, headers=headers, timeout=10)
                if response.status_code != 200:
                    if page == 1:
                        print(f'   âŒ Category not accessible: {response.status_code}')
                    break
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Find article links
                article_links = []
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if '/story/' in href and 'illawarramercury.com.au' in href:
                        article_links.append(href)
                
                if not article_links:
                    if page == 1:
                        print(f'   âš ï¸ No articles found on page {page}')
                    else:
                        print(f'   ğŸ“„ Page {page}: End of pagination')
                    break
                
                # Check for target articles
                page_found = []
                for link in article_links:
                    for target_id in target_ids:
                        if target_id in link:
                            page_found.append((target_id, link))
                            all_found.append((target_id, link, category, page))
                
                if page_found:
                    print(f'   ğŸ“„ Page {page}: {len(article_links)} articles - ğŸ¯ FOUND {len(page_found)} TARGETS!')
                    for target_id, link in page_found:
                        print(f'      â€¢ {target_id}: {link}')
                else:
                    # Show sample IDs for context
                    story_ids = []
                    for link in article_links[:5]:
                        match = re.search(r'/story/(\d+)/', link)
                        if match:
                            story_ids.append(int(match.group(1)))
                    
                    if story_ids:
                        print(f'   ğŸ“„ Page {page}: {len(article_links)} articles (IDs: {min(story_ids)}-{max(story_ids)})')
                    else:
                        print(f'   ğŸ“„ Page {page}: {len(article_links)} articles')
                
                time.sleep(0.3)  # Rate limiting
                
            except Exception as e:
                print(f'   âŒ Error on page {page}: {str(e)}')
                break
    
    print('\n' + '=' * 60)
    print('ğŸ“Š DEEP SCAN RESULTS')
    print('=' * 60)
    
    if all_found:
        print(f'ğŸ¯ Found {len(all_found)} target articles:')
        for target_id, link, category, page in all_found:
            category_name = category.split('/')[-2] if category.endswith('/') else category.split('/')[-1]
            print(f'   â€¢ {target_id}: Page {page} of {category_name}')
            print(f'     URL: {link}')
    else:
        print('âŒ No target articles found in deep pagination scan')
        print('ğŸ’¡ Articles might be in:')
        print('   â€¢ Search results pages')
        print('   â€¢ RSS feeds')
        print('   â€¢ Dynamic content (JavaScript-loaded)')
        print('   â€¢ Archive sections not in main navigation')
    
    return all_found

if __name__ == "__main__":
    found_articles = scan_deep_pagination()
